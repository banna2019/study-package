
tar xf prometheus-2.9.2.linux-amd64.tar.gz
mv prometheus-2.9.2.linux-amd64 /etc/prometheus
cp /etc/prometheus/promtool /usr/bin/
cp /etc/prometheus/prometheus /usr/bin/


Prometheus启动脚本:
cat >> /usr/lib/systemd/system/prometheus.service << EOF
[Unit]
Description=Prometheus
Wants=network-online.target
After=network-online.target

[Service]
User=prometheus
Group=prometheus
Type=simple
ExecStart=/usr/local/bin/prometheus \\
    --config.file /etc/prometheus/prometheus.yml \\
    --storage.tsdb.path=/data/prometheus/ \\
    --storage.tsdb.retention=30d \\   
    --web.console.templates=/etc/prometheus/consoles \\
    --web.enable-lifecycle \\
    --web.console.libraries=/etc/prometheus/console_libraries

Restart=on-failure

[Install]
WantedBy=multi-user.target
EOF


chmod 777 /usr/lib/systemd/system/prometheus.service


curl  -XPOST localhost:9090/-/reload


tar xf node_exporter-0.17.0.linux-amd64.tar.gz 
mv node_exporter-0.17.0.linux-amd64 /etc/node_exporter
cp /etc/node_exporter/node_exporter  /usr/bin/


cat >> /usr/lib/systemd/system/node_exporter.service << EOF
[Unit]
Description=Prometheus node_exporter
Documentation=https://prometheus.io/
Wants=network-online.target
After=network-online.target

[Service]
Type=simple
User=prometheus
Group=prometheus
ExecStart=/etc/node_exporter/node_exporter \\
    --collector.systemd \\
    --log.level=error
	
ExecStop=/usr/bin/killall node_exporter
Restart=on-failure
MemoryLimit=300M
CPUQuota=100%

[Install]
WantedBy=multi-user.target
EOF


chmod 777 /usr/lib/systemd/system/node_exporter.service


文本文件收集器:
	mkdir -pv /var/lib/node_exporter/textfile_collector
	echo 'metadata{role="docker_server",datacenter="NJ"}1'|sudo tee /var/lib/node_exporter/textfile_collector/metadata.prom

	备注: 文本文件收集器,默认是加载的是需要指定的目录 --collector.textfile.directory=""
	
	
	启用system收集器:
		采用 --collector.systemd.unit-whitelist=".+"		(使用正则表达式)
		

	运行节点导出器:
		参照:
		nohup node_exporter --collector.textfile.directory /var/lib/node_exporter/textfile_collector --collector.systemd --collector.systemd.unit-whitelist="{docker|sshd|rsyslog}.service" --web.listen-address="0.0.0.0:9700" --web.telemetry-path="/node_metrics" > node_exporter.out 2>&1 &
		实际使用:
		nohup node_exporter --collector.textfile.directory /var/lib/node_exporter/textfile_collector --collector.systemd --collector.systemd.unit-whitelist="{docker|sshd|rsyslog}.service" > node_exporter.out 2>&1 &

		备注: 默认监听9100
			  创建文件收集器、启用systemd,然后运行node_exporter


在服务器上过滤收集器:
curl https://raw.githubusercontent.com/aishangwei/prometheus-demo/master/prometheus/prometheus.yml

	备注: 使用params参数配置collect,过滤想要的数据
	curl -g -X GET http://192.168.2.57:9100/metrics?collect[]=cpu
	
	

cadvisor_run.sh:	
curl https://raw.githubusercontent.com/aishangwei/prometheus-demo/master/docker/cadvisor_run.sh
wget https://raw.githubusercontent.com/aishangwei/prometheus-demo/master/docker/cadvisor_run.sh

curl http://192.168.2.57:8080
curl http://192.168.2.57:8080/metrics


curl http://192.168.2.57:9090




计算CPU5分钟平均使用比率:
	irate(node_cpu_seconds_total{job="node",mode="idle"}[5m]))
	avg(irate(node_cpu_seconds_total{job="node",mode="idle"}[5m])) by (instance) 
	avg(irate(node_cpu_seconds_total{job="node",mode="idle"}[5m])) by (instance) * 100

	100 - avg(irate(node_cpu_seconds_total{job="node",mode="idle"}[5m])) by (instance) * 100


CPU Saturation(饱和度):
	count by (instance)(node_cpu_seconds_total{mode="idle"})


内存使用:
	node_memory_MemTotal_bytes 		主机上的总内存
	node_memory_MemFree_bytes		主机上的空闲内存
	node_memory_Buffers_bytes		缓冲区缓冲中的内存
	node_memory_Cached_bytes		页面缓冲中的内存

	没存使用比率:
		(node_memory_MemTotal_bytes -(node_memory_MemFree_bytes+node_memory_Cached_bytes+node_memory_Buffers_bytes)) / node_memory_MemTotal_bytes * 100
		

磁盘监控:
	node_filesystem_size_bytes 	(显示正在被监控的每个文件系统挂载的大小,可以使用类似于内存度量的查询来生成主机上磁盘空间的使用百分比)
	
	(node_filesystem_size_bytes{mountpoint="/"} - node_filesystem_free_bytes{mountpoint="/"}) / node_filesystem_size_bytes{mountpoint="/"} * 100
	
	(node_filesystem_size_bytes{mountpoint="/data"} - node_filesystem_free_bytes{mountpoint="/data"}) / node_filesystem_size_bytes{mountpoint="/data"} * 100  #磁盘挂载点监控
	(node_filesystem_size_bytes{mountpoint=~"/|/data"} - node_filesystem_free_bytes{mountpoint=~"/|/data"}) / node_filesystem_size_bytes{mountpoint=~"/|/data"} * 100  #多个磁盘挂载点监控
	
	
	备注:
		1.不能使用匹配空字符串的正则表达式
		2.对于不匹配的正则表达式,还有一个!~运算符
		
	
	预计多长时间磁盘爆满:
	 predict_linear(node_filesystem_free_bytes{mountpoint="/"}[1h],4*3600) < 0
	 predict_linear(node_filesystem_free_bytes{job="node"}[1h],4*3600) < 0
	
		#一旦小于就会返回一个负数,然后触发报警 predict_linear 函数
		


监控服务状态:
	systemd收集器的数据,目前只收集了Docker SSH和Rsyslog
		
	node_systemd_unit_state{name="docker.service"}		//只查询docker服务
	node_systemd_unit_state{name="docker.service",state="active"}	//返回活动状态
	node_systemd_unit_state{name="docker.service"} == 1		//返回当前服务的状态
	
	备注: 比较二进制运算符: ==. 这将检索所有值为1、名称标签为docker.service的指标
	
	
	
up metric:
	Execute: up 
	
	监视特定节点状态的另一个有用指标: up,如果实例时健康的,度量就被设置为1,失败返回-或0
	
	备注:
		许多exporter都有专门的指标来识别服务的最后一次成功的scape
		如:
			cAdvisor  container_last_seen
			MySQL     mysqlup
			
			
Metadata metric:
	使用节点导出器的Textfile收集器创建的指标元数据
	
	metadata{role="docker_server",datacenter="NJ"}1
	metadata{datacenter != "NJ"}
	
	
	算术和比较二进制运算符:
		https://prometheus.io/docs/prometheus/latest/querying/operators/
	
	
	
Vector matches(向量配置):
	向量配置方式:
		一对一匹配:
			node_systemd_unit_state{name="docker.service"} == 1 and on (instance,job)metadata{datacenter="BJ"}
			
		多对一或一对多方式:
			多对一和一对多匹配是指"one"一侧的每个向量元素可以与"many"一侧的多个元素匹配.使用group_left或group_right修饰符显式地
			指定这些匹配,其中left或right决定了哪个向量觉有更高的基数
			
			参考地址: https://prometheus.io/docs/prometheus/latest/querying/operators/#vector-matching
		
		
			
持久查询:
	持久查询的三种方法:	
		1).记录规则  -- 从查询中创建新的指标
		
			记录规则是一种计算新时间序列的方法,特别是从输入的时间序列中聚合的时间序列:
			
				跨多个时间序列生成聚合
				预计算安规的查询,即消耗大量时间或计算能力的查询
				生成一个时间序列,可以用他来生成警报
				
		2).警报规则  -- 从查询生成警报
			记录规则存在Prometheus服务器上,存在Prometheus服务器加载的文件中.规则是自动计算的,频率有prometheus.yml全局块中的evaluation_interval参数控制
			
			创建rule文件:
				mkdir -pv /usr/local/prometheus/rules
				touch /usr/local/prometheus/rules/node_rules.yml
				
				vim /usr/local/prometheus/prometheus.yml
				
					rule_files:
					   - "rules/node_rules.yml"
					   
			添加记录规则:
				命名规则建议:
					level:metric:operations
					例如: instance:node_cpu:avg_rate5m
					
				vim /usr/local/prometheus/rules/node_rules.yml
				
groups:
- name: node_rules
  interval: 10s
  rules:
  - record: instance:node_cpu:avg_rate5m
    expr: 100 - avg(irate(node_cpu_seconds_total{job="node",mode="idle"}[5m])) by (instance) * 100
	labels:
	  metric_type: aggregation
					
					
				Prometheus WEB命令行执行如下命令:
					instance:node_cpu:avg_rate5m
			
		3).可视化    -- 使用想Grafana这样的仪表盘来可视化查询
			普罗米修斯一般不用于长期数据保留;默认值是15天的时间序列.普罗米修斯关注更直接的监控问题,而不是可视化和仪表板更重要的其他系统.
			使用表达式浏览器、绘制Prometheus用户界面内部的图形以及构建相应的警报,这些方法往往比构建广泛的仪表盘更能体现普罗米修斯时间序列
			数据的实用性.
			
			
			Grafana安装:			
		
cat >/etc/yum.repos.d/grafana.repo<-EOF 
[grafana]
name=grafana
baseurl=https://packages.grafana.com/oss/rpm
repo_gpgcheck=0
enabled=1
gpgcheck=0
gpgkey=https://packages.grafana.com/gpg.key
sslverify=0
sslcacert=/etc/pki/tls/certs/ca-bundle.crt
EOF

			yum makecache
			yum install grafana -y
			
			systemctl start grafana-server.service  
			
			open http://192.168.2.57:3000
			
			
			
服务发现:
	手动配置监控目标显然不适合大批量监控节点,特别是kubernetes
	prometheus通过服务发现解决这个问题,自动机制来检测,分类和识别新的和变更的目标.可以通过以下三种方式:
	
		1).通过配置管理工具填充的文件接收目标列表
			基于文件发现:
				基于文件的发现只是比静态配置更高级的一小步,但是它是对于配置管理工具的配置非常有用.通过基于文件的发现,普罗米修斯一般不用于长期数据保留
				修斯使用文件中指定的目标.这些文件通常由另一个系统生成,例如配置管理系统,如Puppet、Ansible,或者从另一个(如CMDB)
				查询.定期运行脚本或查询,或触发它们(重新)填充这些文件.然后,普罗米修斯按照指定的时间表从这些文件中重新加载目标.
				
				这些文件可以是YAML或JSON格式,并包含定义的目标列表,就像在静态配置中定义它们一样.
				
				mkdir  /usr/local/prometheus/targets/{nodes,docker}
				
				vim /usr/local/prometheus/prometheus.yml 
				
				scrape_configs:
				 - job_name: 'prometheus'
				   static_configs:
				   - targets: ['localhost:9090']
				 - job_name: 'node'
				   file_sd_configs:
				    - files:
					  - targets/nodes/*.json
					  refresh_interval: 5m
				 - job_name: 'docker'
				   file_sd_configs:
				    - files:
					  - targets/docker/*.json
					  refresh_interval: 5m
				
				备注: 也可以使用该参数: prometheus_sd_file_seconds 最近一次更新发现文件的时间
				
cat /usr/local/prometheus/targets/nodes/nodes.json
[{
  "targets":[
  "192.168.2.56:9100",
  "192.168.2.57:9100",
  "192.168.2.58:9100"
  ]
}]


cat /usr/local/prometheus/targets/docker/daemons.json
[{
  "targets":[
  "192.168.2.56:8080",
  "192.168.2.57:8080",
  "192.168.2.58:8080"
  ],
  "labels":{
    "datacenter":"nj"
  }
}]

				//也可以使用下面的方式(YAML):
cat /usr/local/prometheus/targets/nodes/nodes.json
-targets:
 - "192.168.2.56:8080"
 - "192.168.2.57:8080"
 - "192.168.2.58:8080"
 
				
		2).查询API(如Amazon AWS API)以获取目标列表
		3).使用DNS记录返回目标列表
			DNS服务发现:
				DNS服务发现依赖于查询A、AAAA或SRV DNS记录.
				
				//基于SRV记录发现
				scrape_configs:
				 - job_name: 'webapp'
				 dns_sd_configs:
				   - names: ['_prometheus._tcp.xiodi.cn']
				   
				备注: _prometheus 为服务名称,_tcp为协议,xiodi.cn为域名
				
				
				//基于A记录
				- job_name: 'webapp'
				   dns_sd_configs:
				    - names: ['c720174.xiodi.cn']
					  type: A 
					  port: 9090
